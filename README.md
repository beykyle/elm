# East Lansing Model
The East Lansing model is an uncertainty quantified global optical potential. This repo contains the code in the form of notebooks to generate the prior, curate the set of experimental constraints, formulate a likelihood model, run the calibration, and visualize and diagnose the results, all using the package [`rxmc`](https://github.com/beykyle/rxmc). Of course, it also contains the implementation of the physics of the model, in `src/elm/elm.py`.

Feel free to make your own branch or fork of the model to try out your own ideas! This setup should make it easy to implement and test any uncertainty quantified global optical potential.

## publication

The model is described in the following publications:

## install for development or modification

To modify the model, first clone and build
```bash
git clone git@github.com:beykyle/elm.git
cd elm
python3 -m build
```

Then install an editable version locally like so:

```
pip install -ve .
```

Note that `pip` will install package dependencies listed in `requirements.txt`. It is **highly recommended** that you use an isolated virtual environment (e.g. using [venv](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/) or [conda/mamba](https://mamba.readthedocs.io/en/latest/)), as this action will install all of the dependencies in `requirements.txt`, at the specific version required.

If you don't want to create an isolated environment for `elm`, but also don't want `pip` to overwrite the package versions you have with the ones in `requirements.txt`, you can

```
pip install -ve --no-deps .
```
This will require that your current python environment satisfies the `requirements.txt`. 

## Setting up the calibration

Run the notebooks in the following order:

1. `data/curate_elastic_dataset_from_exfor.ipynb` sets up the reactions and observables of interest and uses `exfor_tools` to parse the [Exfor data base](https://www-nds.iaea.org/exfor/)  for experimental data
2. `data/precomputation.ipynb` sets up the `jitr` Workspaces relevant for the reaction data chosen, precomputing model-independent stuff
3. `prior/generate_prior.ipynb` encodes and generates samples from the model prior
4. `calibration/setup_cal.ipynb` builds the data structures necessary to run `mcmc` with newly created prior and set of experimental constraints 

## Running a calibration

Once you've set up your calibration (e.g. by stepping through the notebooks in `prior/`, `data/` and `calibration/`), you can run `mcmc` to run the actual calibration:

```
mpirun -n 1 python -m mpi4py mcmc.py --help
```

For example, once you've run `calibration/setup_cal.ipynb` to generate the `Corpus` objects, you can then run, for example

```
mpirun -n 12 mcmc --nsteps 10000 --burnin 1000  --corpus_path ./calibration/nn_corpus.pkl --prior_path ./prior/prior_distribution.pickle
```

which will run Metropolis-Hastings with twelve independent chains, 10000 steps each using the supplied prior and corpus of constraints.

## visualizing the results

Once a set of samples has been generated by running `mcmc`, the other notebooks in `calibration/` can be used to visualize and diagnose the calibration results, and propagate the posterior back into the data set. The other notebooks in `data/` similarily demonstrate how to propagate the prior into the data set, as well as other models that are built into `jitr`.
